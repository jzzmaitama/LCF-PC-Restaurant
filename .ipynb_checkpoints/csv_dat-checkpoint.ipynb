{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db05a290a2fb23cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:33:55.495716Z",
     "start_time": "2024-06-23T17:33:54.606937Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Annotated ABSA with Emotions Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13756aa288d4e200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:33:56.187075Z",
     "start_time": "2024-06-23T17:33:55.850061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4832 entries, 0 to 4831\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     2414 non-null   float64\n",
      " 1   Review Sentence        4832 non-null   object \n",
      " 2   Aspect term            4832 non-null   object \n",
      " 3   polarity               4832 non-null   object \n",
      " 4   from                   4832 non-null   int64  \n",
      " 5   to                     4832 non-null   int64  \n",
      " 6   Anger                  4830 non-null   float64\n",
      " 7   Disgust                4831 non-null   float64\n",
      " 8   Fear                   4826 non-null   float64\n",
      " 9   Joy                    4815 non-null   float64\n",
      " 10  Sadness                4830 non-null   float64\n",
      " 11  Surprise               4823 non-null   float64\n",
      " 12  Emotion Class          4832 non-null   object \n",
      " 13  emotion context words  4671 non-null   object \n",
      "dtypes: float64(7), int64(2), object(5)\n",
      "memory usage: 528.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bcee935323a96f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:33:57.118662Z",
     "start_time": "2024-06-23T17:33:57.089753Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_emotions(emotion):\n",
    "    if emotion in ['Anger', 'Disgust', 'Fear', 'Sadness']:\n",
    "        return 'Anger'\n",
    "    elif emotion == 'Joy':\n",
    "        return 'Joy'\n",
    "    else:  # 'Surprise'\n",
    "        return 'Surprise'\n",
    "\n",
    "# Apply the function to the 'Emotion Class' column\n",
    "data['Emotion Class'] = data['Emotion Class'].apply(merge_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63797c5984a2cf85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:34:03.643108Z",
     "start_time": "2024-06-23T17:34:03.579021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion Class\n",
       "Joy         3132\n",
       "Anger       1501\n",
       "Surprise     199\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Emotion Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d7f9804d201a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:34:23.222073Z",
     "start_time": "2024-06-23T17:34:23.201514Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the Polarity  row with the value \"conflict\"\n",
    "data = data[data['polarity'] != 'conflict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "450741f6e99d731d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:34:26.710353Z",
     "start_time": "2024-06-23T17:34:26.691645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity\n",
       "positive    2891\n",
       "negative    1003\n",
       "neutral      833\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:34:31.893116Z",
     "start_time": "2024-06-23T17:34:31.875309Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_all_aspects(data):\n",
    "    aspect_dict = {}\n",
    "    for _, row in data.iterrows():\n",
    "        sentence = row['Review Sentence']\n",
    "        aspect = row['Aspect term']\n",
    "        if sentence not in aspect_dict:\n",
    "            aspect_dict[sentence] = []\n",
    "        aspect_dict[sentence].append(aspect)\n",
    "    return aspect_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69858c8f2c8e300e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:34:32.992020Z",
     "start_time": "2024-06-23T17:34:32.969731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "list(string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3aa4597bf8327e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:12:26.805510Z",
     "start_time": "2024-06-23T17:12:26.641176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Review Sentence</th>\n",
       "      <th>Aspect term</th>\n",
       "      <th>polarity</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Emotion Class</th>\n",
       "      <th>emotion context words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3121.0</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Anger</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2777.0</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>57</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>which was above average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1634.0</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>uniformly exceptional, very capable proudly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1634.0</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>uniformly exceptional very capableproudly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1634.0</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>141</td>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>uniformly exceptionalvery capableproudly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4827</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Each table has a pot of boiling water sunken i...</td>\n",
       "      <td>pot of boiling water</td>\n",
       "      <td>neutral</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Anger</td>\n",
       "      <td>sunken into its surface(1)you get platters of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4828</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Each table has a pot of boiling water sunken i...</td>\n",
       "      <td>meats</td>\n",
       "      <td>neutral</td>\n",
       "      <td>99</td>\n",
       "      <td>104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>sunken into its surface(1)you get platters of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4829</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Each table has a pot of boiling water sunken i...</td>\n",
       "      <td>vegetables</td>\n",
       "      <td>neutral</td>\n",
       "      <td>114</td>\n",
       "      <td>124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>sunken into its surface(1)you get platters of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4830</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Each table has a pot of boiling water sunken i...</td>\n",
       "      <td>rice</td>\n",
       "      <td>neutral</td>\n",
       "      <td>130</td>\n",
       "      <td>134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>sunken into its surface(1)you get platters of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Each table has a pot of boiling water sunken i...</td>\n",
       "      <td>glass noodles</td>\n",
       "      <td>neutral</td>\n",
       "      <td>139</td>\n",
       "      <td>152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>sunken into its surface(1)you get platters of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4727 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    Review Sentence  \\\n",
       "0     3121.0               But the staff was so horrible to us.   \n",
       "1     2777.0  To be completely fair, the only redeeming fact...   \n",
       "2     1634.0  The food is uniformly exceptional, with a very...   \n",
       "3     1634.0  The food is uniformly exceptional, with a very...   \n",
       "4     1634.0  The food is uniformly exceptional, with a very...   \n",
       "...      ...                                                ...   \n",
       "4827     NaN  Each table has a pot of boiling water sunken i...   \n",
       "4828     NaN  Each table has a pot of boiling water sunken i...   \n",
       "4829     NaN  Each table has a pot of boiling water sunken i...   \n",
       "4830     NaN  Each table has a pot of boiling water sunken i...   \n",
       "4831     NaN  Each table has a pot of boiling water sunken i...   \n",
       "\n",
       "               Aspect term  polarity  from   to  Anger  Disgust  Fear  Joy  \\\n",
       "0                    staff  negative     8   13    4.0      1.0   1.0  0.0   \n",
       "1                     food  positive    57   61    0.0      0.0   0.0  2.0   \n",
       "2                     food  positive     4    8    0.0      0.0   0.0  4.0   \n",
       "3                  kitchen  positive    55   62    0.0      0.0   0.0  4.0   \n",
       "4                     menu   neutral   141  145    0.0      0.0   0.0  4.0   \n",
       "...                    ...       ...   ...  ...    ...      ...   ...  ...   \n",
       "4827  pot of boiling water   neutral    17   37    2.0      2.0   0.0  1.0   \n",
       "4828                 meats   neutral    99  104    0.0      0.0   0.0  3.0   \n",
       "4829            vegetables   neutral   114  124    0.0      0.0   0.0  2.0   \n",
       "4830                  rice   neutral   130  134    0.0      0.0   0.0  2.0   \n",
       "4831         glass noodles   neutral   139  152    0.0      0.0   0.0  2.0   \n",
       "\n",
       "      Sadness  Surprise Emotion Class  \\\n",
       "0         3.0       1.0         Anger   \n",
       "1         0.0       0.0           Joy   \n",
       "2         0.0       0.0           Joy   \n",
       "3         0.0       0.0           Joy   \n",
       "4         0.0       0.0           Joy   \n",
       "...       ...       ...           ...   \n",
       "4827      0.0       0.0         Anger   \n",
       "4828      0.0       1.0           Joy   \n",
       "4829      0.0       1.0           Joy   \n",
       "4830      0.0       1.0           Joy   \n",
       "4831      0.0       1.0           Joy   \n",
       "\n",
       "                                  emotion context words  \n",
       "0                                              horrible  \n",
       "1                               which was above average  \n",
       "2           uniformly exceptional, very capable proudly  \n",
       "3             uniformly exceptional very capableproudly  \n",
       "4              uniformly exceptionalvery capableproudly  \n",
       "...                                                 ...  \n",
       "4827  sunken into its surface(1)you get platters of ...  \n",
       "4828  sunken into its surface(1)you get platters of ...  \n",
       "4829  sunken into its surface(1)you get platters of ...  \n",
       "4830  sunken into its surface(1)you get platters of ...  \n",
       "4831  sunken into its surface(1)you get platters of ...  \n",
       "\n",
       "[4727 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b84e08ce50be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:12:33.564623Z",
     "start_time": "2024-06-23T17:12:32.668701Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(data, test_size=0.20, random_state=42,shuffle=True)\n",
    "\n",
    "# Define the target variable for the training data\n",
    "y_train = train_df['Emotion Class']\n",
    "\n",
    "# Instantiate the RandomOverSampler\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "# Resample the training dataset\n",
    "X_resampled, y_resampled = ros.fit_resample(train_df, y_train)\n",
    "\n",
    "# Now, 'X_resampled' is your DataFrame with balanced 'Emotion Class' for the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdfbe328dc692745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:35:41.903652Z",
     "start_time": "2024-06-23T17:35:41.863474Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "excluded_words = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "\n",
    "\n",
    "# new implementation\n",
    "def nltk_format_data(row, all_aspects):\n",
    "    # Replace NaN values with empty strings\n",
    "    row = row.fillna('')\n",
    "    \n",
    "    # Remove numbers\n",
    "    sentence = re.sub(r'\\(\\d+\\)', '', row['Review Sentence'])\n",
    "\n",
    "    # Tokenize the sentence while correctly handling punctuation\n",
    "    tokens = word_tokenize(sentence)\n",
    "    current_aspect = row['Aspect term']\n",
    "    polarity_code = {'positive': 2, 'neutral': 0, 'negative': 1}[row['polarity']]\n",
    "\n",
    "    # Map emotion classes to numerical codes\n",
    "    # emotion_code = {'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Joy': 3, 'Sadness': 4, 'Surprise': 5}\n",
    "    emotion_code = {'Anger': 1, 'Joy': 2, 'Surprise': 0}\n",
    "    \n",
    "    # Get all aspects for the current sentence\n",
    "    all_aspects_in_sentence = all_aspects[row['Review Sentence']]\n",
    "\n",
    "    # Normalize the tokens to ensure consistent matching with aspect terms\n",
    "    normalized_tokens = [token.rstrip('.,?!:;') for token in tokens]\n",
    "\n",
    "    # Get the emotion words for the current sentence\n",
    "    emotion_words = word_tokenize(row['emotion context words'])  \n",
    "    emotion_words = [word for word in emotion_words if word not in excluded_words]\n",
    "\n",
    "    formatted_sentence = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        normalized_token = normalized_tokens[i]\n",
    "        matched_aspect = None\n",
    "\n",
    "        # Check if the token is part of any aspect term\n",
    "        for aspect in all_aspects_in_sentence:\n",
    "            aspect_tokens = aspect.split()\n",
    "            aspect_length = len(aspect_tokens)\n",
    "            if normalized_tokens[i:i+aspect_length] == aspect_tokens:\n",
    "                matched_aspect = aspect\n",
    "                break\n",
    "\n",
    "        if matched_aspect:\n",
    "            aspect_tokens = matched_aspect.split()\n",
    "            for j, aspect_token in enumerate(aspect_tokens):\n",
    "                if j == 0:\n",
    "                    formatted_sentence.append(f\"{aspect_token} B-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "                else:\n",
    "                    formatted_sentence.append(f\"{aspect_token} I-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "            i += len(aspect_tokens)  # Skip the aspect tokens\n",
    "        else:\n",
    "            # Check if the token is part of the emotion words\n",
    "            if normalized_token in emotion_words:\n",
    "                formatted_sentence.append(f\"{tokens[i]} O -1 {emotion_code[row['Emotion Class']]}\")\n",
    "            else:\n",
    "                formatted_sentence.append(f\"{tokens[i]} O -1 -1\")\n",
    "            i += 1\n",
    "\n",
    "    return '\\n'.join(formatted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4323e3aa9a5ca5ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:35:53.220836Z",
     "start_time": "2024-06-23T17:35:42.891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       But O -1 -1\\nthe O -1 -1\\nstaff B-ASP 1 1\\nwas...\n",
      "1       To O -1 -1\\nbe O -1 -1\\ncompletely O -1 -1\\nfa...\n",
      "2       The O -1 -1\\nfood B-ASP 2 2\\nis O -1 -1\\nunifo...\n",
      "3       The O -1 -1\\nfood B-ASP -1 -1\\nis O -1 -1\\nuni...\n",
      "4       The O -1 -1\\nfood B-ASP -1 -1\\nis O -1 -1\\nuni...\n",
      "                              ...                        \n",
      "4827    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4828    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4829    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4830    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "4831    Each O -1 -1\\ntable B-ASP -1 -1\\nhas O -1 -1\\n...\n",
      "Length: 4727, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get all aspects for each sentence\n",
    "all_aspects = get_all_aspects(data)\n",
    "\n",
    "# Apply NLTK formatting to the DataFrame and display the results\n",
    "example_data_nltk_formatted = data.apply(nltk_format_data, axis=1, all_aspects=all_aspects)\n",
    "print(example_data_nltk_formatted)  # Displaying formatted data for the first entry\n",
    "# print(example_data_nltk_formatted.values[1])  # Displaying formatted data for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bca1e785a947da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T13:13:05.610675Z",
     "start_time": "2024-06-12T13:13:05.596206Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ada652ad15304bf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T13:15:46.546463Z",
     "start_time": "2024-06-12T13:15:46.529522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14916"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a08512e6eef041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:20:05.586369Z",
     "start_time": "2024-06-23T17:20:03.912903Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(example_data_nltk_formatted, test_size=0.30, random_state=42, shuffle=True)\n",
    "\n",
    "# # Calculate the number of rows for training and testing\n",
    "# train_size = int(0.80 * len(example_data_nltk_formatted))\n",
    "# test_size = len(example_data_nltk_formatted) - train_size\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# train_df = example_data_nltk_formatted.iloc[:train_size]\n",
    "# test_df = example_data_nltk_formatted.iloc[train_size:]\n",
    "\n",
    "\n",
    "# Add a new line at the end of each sentence\n",
    "train_df = train_df.apply(lambda x: x + '\\n' if isinstance(x, str) else x)\n",
    "test_df = test_df.apply(lambda x: x + '\\n' if isinstance(x, str) else x)\n",
    "\n",
    "# Convert and save the training and testing data without quotes\n",
    "train_df.to_csv('Restaurants.atepc.train.dat', index=False, header=False, sep='\\t', quoting=csv.QUOTE_NONE,escapechar=\"\\t\")\n",
    "test_df.to_csv('Restaurants.atepc.test.dat', index=False, header=False, sep='\\t', quoting=csv.QUOTE_NONE,escapechar=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9dc7c12b97bed18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:08:46.510585Z",
     "start_time": "2024-06-23T17:08:46.502009Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1b1ceae1b69716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T17:08:51.011682Z",
     "start_time": "2024-06-23T17:08:47.343417Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Negative'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m all_aspects \u001b[38;5;241m=\u001b[39m get_all_aspects(data)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Apply NLTK formatting to the DataFrame and display the results\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m example_data_nltk_formatted \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnltk_format_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_aspects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_aspects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(example_data_nltk_formatted)  \u001b[38;5;66;03m# Displaying formatted data for the first entry\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# print(example_data_nltk_formatted.values[1])  # Displaying formatted data for \u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10361\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10347\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10349\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10351\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10359\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10360\u001b[0m )\n\u001b[1;32m> 10361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\PycharmProjects\\HealthRiskPredictor\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[22], line 48\u001b[0m, in \u001b[0;36mnltk_format_data\u001b[1;34m(row, all_aspects)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, aspect_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aspect_tokens):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 48\u001b[0m         formatted_sentence\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maspect_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m B-ASP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolarity_code\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mmatched_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mcurrent_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43memotion_code\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEmotion Class\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mmatched_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mcurrent_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m         formatted_sentence\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maspect_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m I-ASP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolarity_code\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mmatched_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mcurrent_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotion_code[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion Class\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mmatched_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mcurrent_aspect\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Negative'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure nltk resources are available\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def get_all_aspects(data):\n",
    "    aspect_dict = {}\n",
    "    for _, row in data.iterrows():\n",
    "        sentence = row['Review Sentence']\n",
    "        aspect = row['Aspect term']\n",
    "        if sentence not in aspect_dict:\n",
    "            aspect_dict[sentence] = []\n",
    "        aspect_dict[sentence].append(aspect)\n",
    "    return aspect_dict\n",
    "def nltk_format_data(row, all_aspects):\n",
    "    # Tokenize the sentence while correctly handling punctuation\n",
    "    tokens = word_tokenize(row['Review Sentence'])\n",
    "    current_aspect = row['Aspect term']\n",
    "    polarity_code = {'positive': 2, 'neutral': 0, 'negative': 1,'conflict':3}[row['polarity']]\n",
    "    \n",
    "    # Map emotion classes to numerical codes\n",
    "    emotion_code = {'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Joy': 3, 'Sadness': 4, 'Surprise': 5} \n",
    "    \n",
    "    # Get all aspects for the current sentence\n",
    "    all_aspects_in_sentence = all_aspects[row['Review Sentence']]\n",
    "    \n",
    "    # Normalize the tokens to ensure consistent matching with aspect terms\n",
    "    normalized_tokens = [token.rstrip('.,?!:;') for token in tokens]\n",
    "    \n",
    "    formatted_sentence = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        normalized_token = normalized_tokens[i]\n",
    "        matched_aspect = None\n",
    "        \n",
    "        # Check if the token is part of any aspect term\n",
    "        for aspect in all_aspects_in_sentence:\n",
    "            aspect_tokens = aspect.split()\n",
    "            aspect_length = len(aspect_tokens)\n",
    "            if normalized_tokens[i:i+aspect_length] == aspect_tokens:\n",
    "                matched_aspect = aspect\n",
    "                break\n",
    "        \n",
    "        if matched_aspect:\n",
    "            aspect_tokens = matched_aspect.split()\n",
    "            for j, aspect_token in enumerate(aspect_tokens):\n",
    "                if j == 0:\n",
    "                    formatted_sentence.append(f\"{aspect_token} B-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "                else:\n",
    "                    formatted_sentence.append(f\"{aspect_token} I-ASP {polarity_code if matched_aspect == current_aspect else '-1'} {emotion_code[row['Emotion Class']] if matched_aspect == current_aspect else '-1'}\")\n",
    "            i += len(aspect_tokens)  # Skip the aspect tokens\n",
    "        else:\n",
    "            formatted_sentence.append(f\"{tokens[i]} O -1 -1\")\n",
    "            i += 1\n",
    "\n",
    "    return '\\n'.join(formatted_sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get all aspects for each sentence\n",
    "all_aspects = get_all_aspects(data)\n",
    "\n",
    "# Apply NLTK formatting to the DataFrame and display the results\n",
    "example_data_nltk_formatted = data.apply(nltk_format_data, axis=1, all_aspects=all_aspects)\n",
    "print(example_data_nltk_formatted)  # Displaying formatted data for the first entry\n",
    "# print(example_data_nltk_formatted.values[1])  # Displaying formatted data for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fc54c1fbeb6970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:36:01.760594Z",
     "start_time": "2024-06-14T14:35:51.598489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] although they do the typical what kind of water would you like questions the service was good and overall very relaxing to place [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "token_ids = [101, 2348, 2027, 2079, 1996, 5171, 2054, 2785, 1997, 2300, 2052, 2017, 2066, 3980, 1996, 2326, 2001, 2204, 1998, 3452, 2200, 19613, 2000, 2173, 102]\n",
    "\n",
    "# Convert token IDs to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# Join tokens to form the original text\n",
    "original_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bc06961d7f778c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:39:16.465076Z",
     "start_time": "2024-06-14T14:39:16.449844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2348, 2027, 2079, 1996, 5171, 2054, 2785, 1997, 2300, 2052, 2017, 2066, 3980, 1996, 2326, 2001, 2204, 1998, 3452, 2200, 19613, 2000, 2173, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"although they do the typical what kind of water would you like questions the service was good and overall very relaxing to place\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717ac3fe5c94d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
